name: "VProfile Actions"
on: workflow_dispatch
env:
    AWS_REGION: "us-east-1"
    ECR_REPOSITORY: "vprofileactionsapp"
    EKS_CLUSTER: "vprofile-eks"
    SONAR_PROJECT_KEY: "vprofile-actions33_vproapp24"
    SONAR_ORGANIZATION: "vprofile-actions33"
    SONAR_URL: "https://sonarcloud.io"
    AWS_ACCOUNT_ID: "227447828999"  # Replace with your AWS Account ID

jobs:
    Testing:
        runs-on: ubuntu-latest
        steps:
        - name: Code Checkout
          uses: actions/checkout@v3
        
        - name: Maven Test
          run: mvn test

        - name: checkstyle
          run: mvn checkstyle:checkstyle

        - name: set up Java 
          uses: actions/setup-java@v3
          with:
              distribution: 'temurin'
              java-version: '11'

        - name: setup sonar scanner
          uses: warchant/setup-sonar-scanner@v7
          
        - name: sonar scan
          uses: SonarSource/sonarcloud-github-action@master
          env:
              GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
              SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

        - name: Check Sonar Quality Gate
          uses: sonarsource/sonarcloud-github-action@master
          env:
              SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
              SONAR_HOST_URL: ${{ env.SONAR_URL }}

    BUILDANDPUBLISH:
        needs: Testing
        runs-on: ubuntu-latest
        steps:
        - name: Code Checkout
          uses: actions/checkout@v3
        - name: Build and Push Docker Image to ECR
          id: build-image
          uses: appleboy/docker-ecr-action@master
          with:
            access_key: ${{ secrets.AWS_ACCESS_KEY_ID }}
            secret_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
            region: ${{ env.AWS_REGION }}
            repo: ${{ env.ECR_REPOSITORY }}
            tags: latest,${{ github.run_number }}
            daemon_off: false
            dockerfile: ./Dockerfile
            context: ./

    DEPLOYTOEKS:
        needs: BUILDANDPUBLISH
        runs-on: ubuntu-latest
        steps:
        - name: Code Checkout
          uses: actions/checkout@v3

        - name: Configure AWS credentials
          uses: aws-actions/configure-aws-credentials@v2
          with:
            aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
            aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
            aws-region: ${{ env.AWS_REGION }}

        - name: Get KubeConfig for EKS Cluster
          run: |
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

        - name: LOGIN to ECR
          run: |
            kubectl delete secret ecr-registry -n default --ignore-not-found=true
            kubectl create secret docker-registry ecr-registry \
              --docker-server=${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com \
              --docker-username=AWS \
              --docker-password=$(aws ecr get-login-password --region ${{ env.AWS_REGION }}) \
              -n default

        - name: Install AWS Load Balancer Controller
          run: |
            # Install eksctl
            curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            
            # Check if controller already exists
            if kubectl get deployment -n kube-system aws-load-balancer-controller >/dev/null 2>&1; then
              echo "AWS Load Balancer Controller exists but appears broken, removing it..."
              # Force delete the broken deployment
              helm uninstall aws-load-balancer-controller -n kube-system || true
              kubectl delete deployment aws-load-balancer-controller -n kube-system --ignore-not-found=true
              kubectl delete serviceaccount aws-load-balancer-controller -n kube-system --ignore-not-found=true
              kubectl delete clusterrole aws-load-balancer-controller --ignore-not-found=true
              kubectl delete clusterrolebinding aws-load-balancer-controller --ignore-not-found=true
              echo "Waiting for cleanup..."
              sleep 30
            fi
            
            # Fresh installation
            echo "Installing AWS Load Balancer Controller from scratch..."
            
            # Create service account with IAM role
            eksctl create iamserviceaccount \
              --cluster=${{ env.EKS_CLUSTER }} \
              --namespace=kube-system \
              --name=aws-load-balancer-controller \
              --role-name AmazonEKSLoadBalancerControllerRole \
              --attach-policy-arn=arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:policy/AWSLoadBalancerControllerIAMPolicy \
              --approve --override-existing-serviceaccounts
            
            # Install controller
            helm repo add eks https://aws.github.io/eks-charts
            helm repo update
            helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
              -n kube-system \
              --set clusterName=${{ env.EKS_CLUSTER }} \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller
            
            # Wait for deployment to be ready
            echo "Waiting for controller to be ready..."
            kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s
            
            # Verify webhook service has endpoints
            echo "Checking webhook service endpoints..."
            kubectl get endpoints aws-load-balancer-webhook-service -n kube-system

        - name: Deploy Helm Chart to EKS
          run: |
            helm upgrade --install vproapp ./vprofilecharts/ \
              --set app.image=${{ secrets.REGISTRY }}/${{ env.ECR_REPOSITORY }} \
              --set app.tag=${{ github.run_number }} \
              --kubeconfig ~/.kube/config \
              --namespace default

        - name: Verify Load Balancer Creation
          run: |
            echo "=== Verifying Load Balancer Setup ==="
            
            # 1. Confirm AWS Load Balancer Controller is running
            echo "1. Checking AWS Load Balancer Controller status..."
            kubectl get deployment aws-load-balancer-controller -n kube-system
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            
            # 2. Verify webhook service has endpoints
            echo "2. Checking webhook service endpoints..."
            kubectl get endpoints aws-load-balancer-webhook-service -n kube-system
            
            # 3. Check ingress resource
            echo "3. Checking ingress resource..."
            kubectl get ingress vpro-ingress -n default -o wide
            kubectl describe ingress vpro-ingress -n default
            
            # 4. Wait for ALB to be provisioned (up to 5 minutes)
            echo "4. Waiting for ALB to be provisioned..."
            for i in {1..30}; do
              ALB_HOST=$(kubectl get ingress vpro-ingress -n default -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
              if [ ! -z "$ALB_HOST" ] && [ "$ALB_HOST" != "null" ]; then
                echo "‚úÖ ALB provisioned successfully!"
                echo "ALB Hostname: $ALB_HOST"
                
                # 5. Get ALB details from AWS
                echo "5. Getting ALB details from AWS..."
                aws elbv2 describe-load-balancers --query 'LoadBalancers[?DNSName==`'$ALB_HOST'`].[LoadBalancerName,State.Code,AvailabilityZones[].ZoneName]' --output table
                
                # 6. Test ALB connectivity
                echo "6. Testing ALB connectivity..."
                curl -I "http://$ALB_HOST" --connect-timeout 10 || echo "ALB not yet responding (normal during initial setup)"
                
                echo "üéâ Load Balancer verification complete!"
                exit 0
              fi
              echo "Waiting for ALB... (attempt $i/30)"
              sleep 10
            done
            
            echo "‚ùå ALB not provisioned within 5 minutes. Checking for issues..."
            kubectl get events -n default --sort-by='.lastTimestamp' | tail -10
            kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=20
            exit 1